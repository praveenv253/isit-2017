\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{array}
\usepackage{enumitem}
\usepackage{color}
\usepackage{url}
\usepackage[colorlinks=true,allcolors=black,urlcolor=blue]{hyperref}

%\usepackage{caption}
%\usepackage{subcaption}

%\captionsetup[figure]{textfont=footnotesize,labelfont=footnotesize}

\graphicspath{{images/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\interdisplaylinepenalty=2500

% Math commands for matrix and vector fonts
\providecommand{\v}{}
\renewcommand{\v}[1]{\underline{#1}}
\providecommand{\vhat}{}
\renewcommand{\vhat}[1]{\underline{\hat{#1}}}
\providecommand{\m}{}
\renewcommand{\m}[1]{{\bf #1}}
\providecommand{\j}{}
\renewcommand{\j}{\jmath}

% Custom math operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Phiorho}{\Phi\!\circ\!\rho}

\renewcommand*\ttdefault{lmtt}

% Theorems, lemmas, etc.
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}   % Shares numbering with theorem
\newtheorem{definition}{Definition}

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

\title{Lower Bounds on the Minimax Risk for the \\ Source Localization Problem}

\author{
	\IEEEauthorblockN{
		Praveen Venkatesh\IEEEauthorrefmark{1}
		and Pulkit Grover\IEEEauthorrefmark{2}
	}
	\IEEEauthorblockA{
		Electrical \& Computer Engineering,
		and the Center for the Neural Basis of Cognition,
		Carnegie Mellon University \\
		\IEEEauthorrefmark{1}\href{mailto:vpraveen@cmu.edu}{\texttt{vpraveen@cmu.edu}}
		\IEEEauthorrefmark{2}\href{mailto:pulkit@cmu.edu}{\texttt{pulkit@cmu.edu}}
	}
}

\begin{document}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}

The ``source localization'' problem is one in which we wish to estimate the
location of a point source observed through a diffusive medium using an array
of sensors. The diffusive medium is modeled as a low-pass filter, and the noise
is taken to be additive, white and gaussian.

We give lower bounds on the minimax risk (mean squared-error in location) in
estimating the location of the source, which applies to all estimators, for
\textcolor{red}{compactly supported (am I using this correctly?)} Lipschitz-continuous impulse responses of
the low-pass filter, when using a unformly distributed sensor array. We show
that the minimax error decays to zero inversely in the number of sensors. We
also perform a preliminary numerical analysis for a more physical model that
views sensors as integrators, wherein the bound saturates before going to zero
for large sensor numbers. The benefit of increased sensor density is seen only
when the signal-to-noise ratio is high. Our bounds are the first to give a
scaling in terms of the number of sensors used.

While our results are shown for sources located on a one-dimensional circular
domain, they can be easily extended to sources on one-dimensional line segments
and d-dimensional boxes. The technique for deriving these minimax bounds also
sheds light on the sensor placement problem: increased resolution in a narrow
region can be obtained by placing sensors \emph{around} the region of interest.

\end{abstract}

\section{Introduction}

%- The source localization problem is one that occurs in several fields
%  - Brain activity measurement, in the form of localizing brain activity
%    - Possible applications are epilepsy focus localization
%    - Neuroscientific experiments - locating the source of activity at certain
%      points in time, for understanding brain function
%  - Need to read Vetterli's papers on acoustic source localization-type things
%  - Need to read up linear inverse problems literature

%- Previous work
%  - In the brain activity localization side of things, lots of algorithms, but
%    not much understanding of bounds (Mosher, Grover).
%    - These have their own issues: CRLB does not apply to all estimators, info
%      theoretic bound does not give scaling with sensors
%  - Other work on minimax bounds and estimators for linear inverse problems
%    concentrate on function estimation (with smoothness constraints on the
%    function being estimated)
%    - e.g. Tsybakov, Efromovich, Ibragimov \& Has'minskii, and references
%      therein
%    - Obviously, bounds for function estimation will look radically different
%      from bounds for a location parameter
%    - Location-parameter based work appears to exist in Ibragimov-Has'minskii,
%      but they treat it as an exercise, without seriously considering
%      extensions, and looking at scaling with number of sensors

%- Cite Fisher's book on circular domain signals somewhere (statistical analysis
%  of circular data)

The source localization problem is one that appears in many fields where it is
of interest to find the position of a source using an array of sensors.

Our principal motivation comes from an application in brain activity
measurement, where one wishes to use modalities such as electroencephalography
(EEG) and magnetoencephalography (MEG) to record activity inside the
brain~\cite{Baillet2001Electromagnetic}.  For example, current EEG systems use
anywhere between 10 and 250 sensors (electrodes) placed on the scalp, to sense
electric potentials produced by neuronal activity within the
brain~\cite{Nunez2006Electric}. The neurons (or groups of neurons) which
produce this activity are frequently modeled as current dipoles. For a single
activated dipole within the brain (which is modeled as being localized to a
point), the electrodes effectively sense a diffuse (blurred-out) representation
of the point source, as a result of the spatial low-pass filtering effect of
the different layers between the brain surface and scalp surface (cerebrospinal
fluid, skull and skin).

Recent developments~\cite{Grover2016Information} have suggested that source
localization can benefit greatly from an increase in sensor density. This
conclusion is based primarily on a Nyquist rate analysis, and on an
information-theoretic bound (which are possibly very loose) on the accuracy of
source localization algorithms. In deriving this bound, however, the scalp
potential was never sampled; equivalently, it was assumed that an
\emph{infinite} number of sensors was available to sample the EEG signal at
every point on the scalp. Hence, the dependence of the bound on the number of
sensors is lost.

Earlier work by Mosher et al.~\cite{Mosher1993Error}, which seeks to address
this issue by deriving Cramer-Rao lower bounds, is also not completely
satisfactory because these bounds do not apply to biased estimators. Commonly
used source localization algorithms (such as the Minimum Norm
Estimate~\cite{Hamalainen1994Interpreting}) on the other hand, are \emph{known}
to be biased~\cite{Lin2006Assessing}.

This paper seeks to address the shortcomings of earlier methods by deriving
minimax lower bounds for the source localization problem, which apply to
\emph{all} estimators (biased \emph{and} unbiased), and which can give a
scaling in terms of the number of sensors used (for a fixed sensor placement
configuration). Computing the lower bound for the ``real brain model'' (where
the complete geometry of the brain surface is accounted for) is rendered very
difficult since analytical solutions for the EEG signal, given the brain
activity, do not exist. ``Spherical head
models''~\cite{Nunez2006Electric,Grover2016Information} \emph{do} have
analytical solutions, but are still not easily amenable to analytical lower
bounds because the spherical surface does not permit uniform
sampling~\cite{Heath1956Thirteen}, and because the solution is expressed in
terms of special functions. We therefore restrict our analysis to a source
localization problem on a one-dimensional domain (to be described in detail in
Section~\ref{sec:source-localization}).

The one-dimensional toy problem is an effective tool for understanding bounds
on source localization accuracy in other settings as well, such as
\textcolor{red}{<insert other applications>}. A vast literature on linear
inverse problems (see~\cite{Bal2012Introduction} for an introduction to this
field) and deconvolution algorithms exists, and has addressed the reduced
one-dimensional problem in broad settings
(see~\cite{Cavalier2002Sharp,Efromovich1997Robust,Ibragimov1981Statistical} and
references therein). However, our setting and interpretation appears to be
unique, since most prior work focuses on recovering a whole function (given
certain smoothness constraints), rather than locating a point source. Work that
does address point sources, to our knowledge, does not address scaling in the
number of sensors, since problem settings and applications differ greatly.

Hence, we believe that this paper is the first to give lower bounds on source
localization error (measured in an $\ell_2$-distance sense) in estimating the
location of a one-dimensional point source, which is observed by sensors
through a diffusive medium (treated as a low-pass filter), and corrupted by
additive Gaussian noise. We describe the problem setting in detail in
Section~\ref{sec:source-localization}, the minimax techniques and our main
result in Section~\ref{sec:minimax-lower-bounds} and a few straightforward
extensions in Section~\ref{sec:extensions}. Finally, we conclude with a
discussion on implications for sensor placement and future work in
Section~\ref{sec:discussion}.

%\textcolor{red}{Introduction needs to be re-written.}

%Electroencephalograhy (EEG) is a system used to record the electrical activity
%of the brain. It is a non-invasive system, which may use anywhere from around
%10 to 250 electrodes placed on the scalp, to sense electric potentials. These
%potentials are generated by neuronal activity within the
%brain~\cite{Buzsaki2012Origin}. These sources of neural activity -- neurons or
%groups of neurons -- are usually modeled as current
%dipoles~\cite{Nunez2006Electric}.

%Often, it is of clinical or scientific interest to reconstruct brain activity
%patterns from EEG sensor measurements. This process is what is broadly referred
%to when we speak of the term ``source localization''. In this paper, I restrict
%the discussion to that of localizing a \emph{single} dipole, and refer to this
%process as ``dipole source localization''. Several algorithms for source
%localization have been proposed over the years
%(see~\cite{Baillet2001Electromagnetic} for an excellent review). However, until
%recently, there has been only one work that has theoretically examined the
%fundamental limits of source localization accuracy.

%Mosher et.\ al.~\cite{Mosher1993Error} first propounded Cramer-Rao lower bounds
%for unbiased source localization algorithms. Their work also forms the basis
%for setting up the source localization problem and deriving more universally
%applicable bounds in this paper. The Cramer-Rao lower bound is not in itself a
%completely satisfactory answer to the question of fundamental limits for source
%localization accuracy. This is because Cramer-Rao bounds only apply to unbiased
%estimators (or to estimators of \emph{known} bias). In practice, however, many
%popular source localization algorithms have been shown to be biased, e.g.\
%the Minimum Norm Estimate (MNE)~\cite{Hamalainen1994Interpreting} is known to
%bias the solution towards the surface of the head~\cite{Lin2006Assessing}. It
%is also known that unbiased estimators can have worse mean-squared-error
%performance than biased estimators. So Cramer-Rao bounds are useful only to a
%degree, and there may be benefit in deriving bounds that hold for all
%estimators.

%To this end, Grover~\cite{Grover2016Fundamental} recently proposed a new lower
%bound for the error in localizing a single dipole. This bound used techniques
%from information theory, for bounding the source localization error by
%expressing it in terms of the mutual information across some channel, and then
%bounding the mutual information by the capacity of the channel. In deriving
%this bound, however, the scalp potential was never sampled; equivalently, it
%was assumed that an \emph{infinite} number of sensors was available to sample
%the EEG signal at every point on the scalp. Clearly, this is a poor assumption
%and could result in a very loose bound, because it allows for more information
%to be available at the receiver than otherwise possible. The lower bound also
%relies on certain relaxations that allow the dipole to distribute its power in
%non-physical ways in frequency domain. This once again makes the bound loose by
%assigning the channel a higher capacity than what it could otherwise have.

%To address the shortcomings of the Cramer-Rao and information theoretic lower
%bounds, I apply two minimax methods to the problem of computing a lower bound
%for localization error in the idealized EEG dipole source localization problem.
%I briefly review two minimax methods -- Le Cam's, and Fano's local
%method~\cite{Duchi2015Information} -- and describe my approach in deriving the
%minimax lower bound using each of these for spherical and one-dimensional head
%models.

\section{The source localization problem}
\label{sec:source-localization}

We start by giving a detailed description of the one-dimensional setting of the
source localization problem. Recall that, in this problem, we're trying to
determine the location of a point source by observing it through a diffusive
medium using a sensor array.

\subsection{Description of the domain}

We assume that the point source is located on a circle of circumference $S$ (a
circular domain enables us to use symmetry arguments to simplify the proof). We
can view this domain as a line, on which signals are periodic with period $S$.
The point source is therefore located somewhere within one period. We denote
the set of possible locations by $\Theta = [0, S)$. Due to the periodic nature
of the domain, a source located at position $\theta \in \Theta$ implies the
presence of sources at $s = \theta + kS$,~$\forall \, k \in \mathbb Z$.

\subsection{Sensor configuration}

Sensors are assumed to be uniformly distributed over the domain, i.e., if there
are $m$ sensors, they are placed at locations $s = 0$, $S/m$, $2S/m$,~\dots,
$(m{-}1)S/m$ (the offset of the first sensor is arbitrary, so without loss of
generality we take it to be 0). The periodicity of the space ensures that we
automatically also have sensors at $S$, $S{+}S/m$,~\dots\@ The lower bounds we
provide, therefore, are for \emph{this specific sensor configuration}. For a
discussion on why this configuration might be an appropriate choice in the
minimax setting, and on non-uniform sensor placement, see
section~\ref{sec:discussion}.

\subsection{Signal model}
\label{sec:signal-model}

All signals on the aforementioned circular domain are of the form
$f:\Theta\mapsto\mathbb{R}$.  The point source located at $\theta$ is
represented by the impulse signal, $f(s;\theta) = \delta(s - \theta)$, where
$\delta(\cdot)$ is the Dirac delta function.  The sensors observe this signal
through the diffusive medium, which, intuitively speaking, blurs the impulse.
More concretely, we assume that the medium is linear and shift-invariant, so
that it can be represented by a spatial impulse response (blurring would then
correspond to low-pass filtering). Let this impulse response be given by
$g(s)$. Then, the noiseless, continuous-space signal (post-filtering and
pre-sampling) is given by the convolution, $x(s; \theta) = (g*f)(s) = g(s -
\theta)$. Here, we further make the simplifying assumption that $g(s)$ has a
sufficiently restricted support, so that aliasing effects are avoided, and
$x(s; \theta)$ is always well-defined. To be precise, $g(s) = 0$ when $\abs{s}
> w/2$, where the ``width'' $w$ of the impulse response satisfies $w < S / 2$
(the reason for the factor of $1/2$ will become clear in a later section).

The sensors sample this continuous-space shifted impulse response, with some
additive noise. We denote the noiseless sampled version of $x(s; \theta)$ by
the $m$-length vector $\v x(\theta)$:
\begin{equation} \label{eq:sampled-signal}
	\v x(\theta) = \bigg[x(0; \theta), \ldots, x\Big(\frac{kS}{m}; \theta\Big), \ldots, x\Big(\frac{(m-1)S}{m}; \theta\Big)\bigg]^T
\end{equation}
where $m$ is the number of sensors and $k \in \{0, 1, \ldots, m-1\}$. The
additive noise is given by $\v \epsilon$ (to be described shortly), and the
noisy samples are represented by $\v y$:
\begin{equation} \label{eq:sensor-obs}
	\v y = \v x(\theta) + \v \epsilon
\end{equation}
The complete signal model described in this section is summarized in
Fig.~\ref{fig:signal-model}.

\subsection{Channel model}
\label{sec:channel-model}

The noise $\v \epsilon$, introduced in equation~\eqref{eq:sensor-obs}, is
the only source of uncertainty in the problem. For our purposes, we assume that
$\v \epsilon$ is zero-mean and Gaussian, $\v \epsilon \sim \mathcal{N}(\v 0, \m
\Sigma)$. Two kinds of covariance matrices are of particular interest. The
first is dubbed the ``sensor noise'' setting, wherein $\m\Sigma = \sigma^2 \m
I$, i.e. the sensors are afflicted by i.i.d.\ noise. The second is called the
``source noise'' setting, wherein $\m\Sigma$ has off-diagonal terms.
\textcolor{red}{Insert description of source noise.}

With the addition of Gaussian noise, each possible source location $\theta$
gives rise to a different distribution at the sensors, denoted by
\begin{equation} \label{eq:p-theta}
	P(\theta) = \mathcal{N}(\v x(\theta), \m \Sigma).
\end{equation}
$\v y$ is therefore one sample from $P(\theta)$. The space of distributions
produced by all possible source locations is $\mathcal{P} = \{P(\theta) :
\theta \in \Theta \}$. We are interested in computing lower bounds for the loss
function $\Phi(\rho(\theta, \hat\theta)) = \abs{\theta - \hat\theta}^2$, where
$\hat\theta$ is some estimate of the location based on $n$ trials (i.e. $n$
realizations) of the noisy sensor observations $\v y$.

\begin{figure}[tp] %  figure placement: here, top, bottom, or page
	\centering
	\includegraphics[width=3.5in]{block-diagram}
	\caption{A diagramatic representation of the signal model described in
	section~\ref{sec:signal-model}.}
	\label{fig:signal-model}
\end{figure}

\section{Minimax lower bounds for the one-dimensional model}
\label{sec:minimax-lower-bounds}

\subsection{Preliminaries}

\textcolor{blue}{
Several techniques for deriving lower bounds on the minimax risk are described
in~\cite{Tsybakov2009Introduction}. We follow the excellent tutorial written by
John Duchi~\cite{Duchi2015Information} to briefly outline the steps involved in
deriving these bounds.
}

Consider the following estimation problem: we have $n$ i.i.d.\ random samples
$Y^n$ from a distribution $P$, which is indexed by a parameter $\theta \in
\Theta$.  Denote the set of these distributions by $\mathcal{P} = \{P(\theta) :
\theta \in \Theta\}$. Suppose we now wish to estimate $\theta$ from $Y^n$.
Define a loss metric $\rho(\theta, \hat\theta)$ (e.g.\ $\rho(\theta,
\hat\theta) = \abs{\theta - \hat\theta}$). For this metric, we can define the
minimax risk over all possible estimators $\hat\theta(Y^n)$ and all possible
$\theta \in \Theta$:
\begin{equation} \label{eq:minimax-expr}
	\mathfrak{M}_n(\mathcal{P}, \Phiorho) = \inf_{\hat\theta} \sup_{\theta \in \Theta} \mathbb E[\Phiorho (\hat\theta(Y^n), \theta)]
\end{equation}
where $\Phi$ is any non-decreasing function (e.g.\ $\Phi(\rho) = \rho^2$) and
$Y^n$ is the random vector corresponding to a noisy realization of sensor
values.

We start by lower bounding the minimax risk of the estimation problem with the
risk incurred in a multiple hypothesis testing problem. For this, we first need
to define a $2\delta$-packing:%
\begin{definition}
	A set $\Theta_{\mathcal{V}} = \{ \theta_v : v \in \mathcal{V} \}$ for some
	finite index set $\mathcal{V} \subset \mathbb N$ is said to be a
	$2\delta$-packing in the $\rho$-metric if
	\begin{equation}
		\rho(\theta_i, \theta_j) \geq 2\delta \qquad \forall \;\; \theta_i, \theta_j \in \Theta_{\mathcal{V}}.
	\end{equation}
\end{definition}
\begin{theorem} \label{thm:est-to-testing}%
	If we can find a $2\delta$-packing $\Theta_{\mathcal{V}}$ of $\Theta$, then
	we can lower bound the minimax estimation risk by the average testing risk:
	\begin{equation}
		\mathfrak{M}_n(\mathcal{P}, \Phiorho) \geq \Phi(\delta) \inf_\psi \mathbb P (\psi(Y^n) \neq V)
	\end{equation}
	where $V$ is the unknown, true hypothesis, and $\psi$ is our estimate of
	the hypothesis.
\end{theorem}
For a proof of this theorem, we refer the reader to Proposition~13.3
in~\cite{Duchi2015Information}.  Intuitively, Theorem~\ref{thm:est-to-testing}
says that the error in estimating $\theta_i$ is likely to be more if it is
difficult to distinguish $\theta_i$ from $\theta_j$, i.e.\ if the probability
of error is high (where $\theta_j$ has been selected to be suitably close).

We now need to lower bound the probability of error in the hypothesis testing
problem. The simplest way to do this is to consider a binary hypothesis test
($\abs{\mathcal{V}} = 2$) and use what is known as Le Cam's method:
\begin{theorem} \label{thm:le-cam}
	For a binary hypothesis test, i.e., when $\mathcal{V} = \{0, 1\}$,
	\begin{equation}
		\inf_\psi \mathbb P(\psi(Y^n) \neq V) = 1 - \norm{P_0 - P_1}_{TV}
	\end{equation}
	where $P_i$ is short-hand for $P(\theta_i)$ and $\norm{P_0 - P_1}_{TV}$ is
	the total variation distance between the two distributions, defined as
	$\norm{\cdot}_{TV} = \frac{1}{2} \norm{\cdot}_1$.
\end{theorem}
For a proof, we refer the reader to Proposition~2.11
in~\cite{Duchi2015Information}.  Intuitively, Theorem~\ref{thm:le-cam} states
that the minimum probability of error that any estimator must make in a binary
hypothesis testing problem is related to the distance between the distributions
corresponding to the two hypotheses. The closer the two distributions, the
higher the chance of making an error in distinguishing between them.

Thus, the final lower bound can be written as:
\begin{equation} \label{eq:le-cam-bound}
	\mathfrak{M}_n(\mathcal P, \Phiorho) \geq \frac{\Phi(\delta)}{2} \big(1 - \norm{P_1^n - P_0^n}_{TV}\big)
\end{equation}
The superscripts ``$n$'' remind us that these are $n$-fold product
distributions, since we have $n$ i.i.d. trials used in the estimate.  The
difficulty in Le Cam's method lies in selecting the two hypotheses to trade-off
the effect due to a small value of $\delta$ and a large value of $\norm{P_1^n -
P_0^n}_{TV}$ appropriately, to derive the tightest possible bound.

\subsection{Lower bounds using Le Cam's method}
\label{sec:lecam-lb}

We now state the main result of this paper.
\begin{theorem} \label{thm:main-lb}
	For a source localization problem as defined in
	section~\ref{sec:source-localization} with sensor noise and a spatial
	impulse response that is $\kappa$-Lipschitz continuous and has restricted
	support of width $w$, the \textcolor{blue}{asymptotic} minimax risk in estimating the
	location of a point source is lower bounded by:
	\begin{equation} \label{eq:main-lower-bound}
		\mathfrak{M}_n(\mathcal{P}, \Phiorho) \geq \frac{1}{32} \frac{\sigma^2 S}{nm\kappa^2w}
	\end{equation}
\end{theorem}

\begin{IEEEproof}
Starting from equation~\eqref{eq:le-cam-bound} and working on the setting
described in Section~\ref{sec:source-localization}, we proceed to derive the
total variation distance for the distributions of interest. Using Pinsker's
inequality~\cite{Kullback1967Lower} and the convenient tensorization of the
KL-divergence~\cite{Duchi2015Information}, we see that:
\begin{equation} \label{eq:pinsker-tensorization}
	\norm{P_1^n - P_0^n}_{TV}^2 \leq \frac{1}{2} D_{KL}(P_0^n \Vert P_1^n) = \frac{n}{2} D_{KL}(P_0 \Vert P_1)
\end{equation}
For multivariate normal distributions with the same covariance, the
KL-divergence is given by
\begin{equation} \label{eq:kl-div-normal}
	D_{KL}(P_0 \Vert P_1) = (\v \mu_0 - \v \mu_1)^T \m \Sigma^{-1} (\v \mu_0 - \v \mu_1)
\end{equation}
where $\v \mu_0$ and $\v \mu_1$ are the means of $P_0$ and $P_1$
respectively~\cite{DuchiDerivation}.

\begin{figure}[t]
	\centering
	\includegraphics[width=2.8in]{overlap-middle-pics}
	\caption{Depiction of the continuous-space signals produced by two
		hypotheses, $\theta_0$ and $\theta_1$. Note that each signal is a
		shifted impulse response, and therefore has a support of size $w$.
		The set $\{\theta_0, \theta_1\}$ is a $2\delta$-packing of
		$\Theta$, so the signals are separated by a distance $2\delta$.}
	\label{fig:overlap-middle}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=2.8in]{delta-sampled-pics}
	\caption{Plot of the samples of the difference signal, $\Delta(k)$. The
		total size of the support of the difference signal is $w+2\delta$,
		hence at most $\floor*{\frac{(w + 2\delta)m}{S}}$ samples of
		$\Delta(k)$ are non-zero.}
	\label{fig:delta-sampled}
\end{figure}

For the case of sensor noise, $P_i = \mathcal{N}(\v x(\theta_i), \sigma^2 \m
I)$, as described in section~\ref{sec:channel-model}. Hence, combining
equations~\eqref{eq:le-cam-bound}, \eqref{eq:pinsker-tensorization} and
\eqref{eq:kl-div-normal}, we see that
\begin{equation} \label{eq:lb-after-pinsker}
	\mathfrak{M}_n(\mathcal{P}, \Phiorho) \geq \frac{\delta^2}{2} \Bigg[ 1 - \sqrt{ \frac{n}{2\sigma^2} \norm*{\v x(\theta_0) - \v x(\theta_1)}^2 } \Bigg]
\end{equation}
Let $\v \Delta \overset{\text{def.}}{=} \v x(\theta_0) - \v x(\theta_1)$ for
brevity. Also, let $\Delta(k)$ denote the $k$-th element of $\v\Delta$, and
$\mathbb I_A(k)$ denote the indicator function of $k$ belonging to the set $A$
(i.e., $\mathbb I_A(k) = 1$ if $k \in A$, and is $0$ otherwise). Then, we have
\begin{align}
	&\norm{\v\Delta}^2 = \sum_{k=0}^{m-1} \abs{\Delta(k)}^2 = \sum_{k=0}^{m-1} \abs{\Delta(k)}^2 \, \mathbb I_{\{\ell: \abs{\Delta(\ell)} > 0\}}(k) \\
	&= \sum_{k=0}^{m-1} \abs[\Big]{x\Big(\frac{kS}{m}; \theta_0\Big) - x\Big(\frac{kS}{m}; \theta_1\Big)}^2 \mathbb I_{\{\ell: \abs{\Delta(\ell)} > 0\}}(k) \\
	&= \sum_{k=0}^{m-1} \abs[\Big]{g\Big(\frac{kS}{m} - \theta_0\Big) - g\Big(\frac{kS}{m} - \theta_1\Big)}^2 \mathbb I_{\{\ell: \abs{\Delta(\ell)} > 0\}}(k)
\end{align}
where $x(s;\theta_i)$ is the continuous-space filtered signal described in
section~\ref{sec:signal-model}. For an impulse response $g$ which is Lipschitz
continuous with parameter $\kappa$, we can upper bound the term within the
summation:
\begin{equation}
	\abs[\Big]{g\Big(\frac{kS}{m} - \theta_0\Big) - g\Big(\frac{kS}{m} - \theta_1\Big)} \leq \kappa \abs{\theta_0 - \theta_1} = \kappa \cdot 2\delta
\end{equation}
since $\abs{\theta_0 - \theta_1} = 2\delta$ by virtue of the $2\delta$ packing.
Hence,
\begin{equation}
	\norm{\v\Delta}^2 \leq 4 \kappa^2 \delta^2 \sum_{k=0}^{m-1} \mathbb I_{\{\ell: \abs{\Delta(\ell)} > 0\}}(k) = 4 \kappa^2 \delta^2 \norm{\v\Delta}_0.
\end{equation}
$\norm{\v\Delta}_0$ is the number of non-zero elements in $\v\Delta$, which is
equal to the number of sensors in the total region covered by the signals
$x(s;\theta_0)$ and $x(s;\theta_1)$ (see Figs.~\ref{fig:overlap-middle} and
\ref{fig:delta-sampled}). Therefore, $\norm{\v\Delta}_0 = \floor[\big]{\frac{(w
+ 2\delta)m}{S} + 1}$, since at most a fraction $(w + 2\delta) / S$ of the $m$
sensors \textcolor{blue}{(plus 1, to account for edge-effects)} can lie in the region covered by
the two impulse responses. The final upper bound on $\norm{\v\Delta}^2$ is
hence
\begin{equation} \label{eq:delta-bound}
	\norm{\v\Delta}^2 \leq 4 \kappa^2 \delta^2 \floor*{\frac{(w + 2\delta)m}{S} + 1} \leq 4 \kappa^2 \delta^2 \Big(\frac{(w + 2\delta) m}{S} + 1\Big).
\end{equation}
Since sensors are uniformly distributed, and since the domain is periodic, this
holds even if $\theta_0$ lies at the edge of the domain (close to $s=0$, for
example). \textcolor{blue}{In such a case, one part of the signal $x(s;\theta_0)$ will appear at
the left edge of the domain, and the remaining part will appear as the
repetition from the period $[S, 2S)$, at the right edge of the domain.} Also
note that the two signals $x(s;\theta_0)$ and $x(s;\theta_1)$ overlap at most
once, since $w < S/2$, as stated in section~\ref{sec:signal-model}.

The upper bound in~\eqref{eq:delta-bound} translates into a lower
bound for~\eqref{eq:lb-after-pinsker}:
\begin{align}
	\mathfrak{M}_n(\mathcal{P}, \Phiorho) &\geq \frac{\delta^2}{2} \Bigg[ 1 - \sqrt{\frac{n \cdot 4 \kappa^2 \delta^2}{2\sigma^2} \bigg( \frac{(w + 2\delta) m}{S} + 1\bigg)} \Bigg] \\
	&= \frac{\delta^2}{2} \Bigg[ 1 - \sqrt{\frac{2n \kappa^2 \delta^2}{\sigma^2} \bigg( \frac{(w + 2\delta) m}{S} + 1\bigg)} \Bigg] \label{eq:lb-after-delta-bound}
\end{align}
Asymptotically, we choose smaller values of $\delta$ as the number of sensors
grows large. Hence, \textcolor{blue}{neglecting edge effects} and terms of order $\delta^3$, we
tighten the bound by choosing $\delta$ as a function of the remaining variables
in order to have $\sqrt{\frac{2nm\kappa^2\delta^2 w}{\sigma^2 S}} =
\frac{1}{2}$. This is achieved for $\delta = \sqrt{\frac{\sigma^2
S}{8nm\kappa^2 w}}$, so that for $\Phi(\delta) = \delta^2$,
equation~\eqref{eq:lb-after-delta-bound} becomes
\begin{equation}
	\mathfrak{M}_n(\mathcal{P}, \Phiorho) \geq \textcolor{blue}{\frac{1}{32}} \frac{\sigma^2 S}{nm\kappa^2 w}.
\end{equation}
This completes the proof.
\end{IEEEproof}

\subsection{Towards a more realistic sensor model}

The bound in Theorem~\ref{thm:main-lb} indicates that the error in source
localization goes to zero, as the number of sensors goes to infinity. This
observation is distinct from the lower bound given
in~\cite{Grover2016Fundamental}, where even with an infinite number of sensors,
the lower bound is strictly greater than zero. This indicates that the bound
in section~\ref{sec:lecam-lb} is loose for large numbers of sensors.

The looseness lies in a poor sensor model: we assume that an arbitrarily large
number of sensors can be squeezed into a finite amount of space, and that these
sensors \emph{still} give us i.i.d.\ observations, with the same noise
variance. This is an impractical expectation. In reality, sensors have a
certain width, and the signal measured by them is the integral of the
continuous-space signal within that width (this is similar to a sample-and-hold
circuit, which integrates a continuous-\emph{time} signal over a short period).
If the width of a sensor is decreased, then effectively, its signal-to-noise
ratio reduces \textcolor{red}{<Need to add reference or explanation>} (this is
intuitively evident in the sample-and-hold analogy: to compute the signal
value, the integral must be divided by the time interval over which the
integration is performed; this division amplifies noise as the time interval
gets smaller).

We performed a preliminary study to understand the effect of viewing sensors
as integrators. The bounding technique used in section~\ref{sec:lecam-lb} no
longer easily admits a solution which gives a bound for all values of $m$. We
can compute the bound numerically, however, using exact expressions for error
probability in a binary hypothesis test. For gaussian distributions of equal
variance, the expression is a Q-function in the distance between their means:
\begin{equation}
	P_e = \inf_\psi \mathbb P(\psi(Y^n) \neq V) = Q\bigg(\frac{\norm{\v\mu_1 - \v\mu_2}}{2\sigma}\bigg)
\end{equation}
\textcolor{red}{Is this definitely correct? i.e.\ the best test is a linear
separator? I think this is coming from Neyman-Pearson. Should I cite?}
The minimax estimation risk is lower bounded by the risk in hypothesis testing,
so
\begin{equation}
	\mathfrak{M}_n(\mathcal{P}, \Phiorho) = \Phi(\delta) P_e = \delta^2 Q\bigg(\frac{\norm{\v x(\theta_0) - \v x(\theta_1)}}{2\sigma}\bigg).
\end{equation}
This bound is valid for any pair of hypotheses $\theta_0$ and $\theta_1$,
separated by a distance of $2\delta$. As the $\delta$ increases, it becomes
easier to distinguish between the two hypotheses, so $P_e$ falls. When
numerically evaluating the bound, therefore, we have to \emph{manually}
trade-off between $\delta$ and $P_e$ to find the tightest possible bound. This
comes down to choosing $\theta_0$ and $\theta_1$ so that they are the hardest
hypotheses to distinguish between. To make the simulation easier, we can relax
the process of finding the least discernable hypothesis pair by fixing
$\theta_0$. Then, we need only optimize over $\delta$, making the process more
tractable. This comes at the cost of loosening the lower bound.

\begin{figure}[t]
	\centering
	\includegraphics[width=3.2in]{lb-vs-numsensors-int}
	\caption{A numerically generated plot of the lower bound on minimax risk of
	source localization, against the number of uniformly distributed sensors
used to estimate the location. The different curves are for different numbers
of ``trials'' (i.i.d.\ sensor observations). The bound saturates for large $m$,
indicating that for a fixed number of trials (or equivalently, for a fixed
SNR), there is reduced benefit in increasing the number of sensors beyond a
point. However, with more trials, the bound saturates at increasingly larger
numbers of sensors. Thus, there is benefit to increasing the number of sensors,
provided SNR can be increased by leveraging larger numbers of trials.}
	\label{fig:numerical}
\end{figure}

Plotting this numerically computed bound for different numbers of trials (see
Fig.~\ref{fig:numerical}) reveals some interesting trends. First, the bound
saturates as $m$ grows large, matching the behaviour presented
in~\cite{Grover2016Fundamental}. Second, the lower bound on error is larger for
smaller numbers of sensors, as we expect. With fewer sensors, it is harder to
estimate the location of the point source. Third and most importantly, the
point at which the bound begins to saturate appears to increase when we use
more trials. Recall that the number of ``trials'' is the number of i.i.d.\
sensor observations we receive. Intuitively, estimation is eased with more
trials, as we could perform more averaging, and increase SNR. \emph{With higher
SNR, there is greater benefit in using a larger number of sensors.} This
matches the intuition given in~\cite{Grover2016Information}, where it is
propounded that using more sensors allows us to sample a greater number of
high-frequency components of the source signal, and hence get a better estimate
of its location.

\section{Extensions}
\label{sec:extensions}

\subsection{From a circular (periodic) domain to a linear (aperiodic) domain}

So far, we've relied on the presence of a circular domain for to make the
arguments in the proof. However, the most important requirement was that
\emph{all} parts of the impulse response should get \emph{sensed} uniformly.
So, if $\theta$ were restricted to the region $[0, S)$ (now on a linear,
non-periodic domain), and if sensors were available to sense the signal from
$[-w/2, S{+}w/2)$ (for an impulse response with restricted support as described
in section~\ref{sec:signal-model}), then the proof for sensor noise would still
fall through, albeit with slightly different constants.

%\subsection{From Lipschitz to $\alpha$-Holder continuous impulse responses}

%We can trivially expand the domain of impulse responses to which the lower
%bound in~\eqref{eq:main-lower-bound} applies to include $\alpha$-Holder
%continuous functions. These are functions which satisfy
%\begin{equation}
%    \abs{g(u) - g(v)} \leq \kappa \abs{u - v}^\alpha \, \forall \, u, v \in \text{dom} \; g
%\end{equation}
%for some constant $\kappa$, and for $\alpha > 0$. Using this property,
%equation~\eqref{eq:delta-bound} reads
%\begin{equation} \label{eq:holder-delta-bound}
%    \norm{\v\Delta}^2 \leq \kappa^2 (2 \delta)^{2\alpha} \bigg(\frac{(w + 2\delta) m}{S} + 1\bigg).
%\end{equation}
%Hence, we choose $\delta = (\frac{\sigma^2 S}{2^{2\alpha + 1} nm \kappa^2
%w})^{1/2\alpha}$, to procure an asymptotic lower bound
%\begin{equation}
%    \mathfrak{M}_n(\mathcal{P}, \Phiorho) \geq \frac{1}{4} \bigg(\frac{\sigma^2 S}{2^{2\alpha + 1} nm \kappa^2
%w}\bigg)^{1/\alpha}.
%\end{equation}

\subsection{From a one-dimensional domain to a $d$-dimensional domain}

Another simple extension takes our problem from a one-dimensional setting to a
$d$-dimensional setting. Here, we would have $\v\theta \in [0, S)^d$, and the
$\ell_2$ loss function, $\Phiorho(\v\theta, \vhat\theta) = \norm{\v\theta -
\vhat\theta}^2$. Then, using Assouad's lemma~\cite{Tsybakov2009Introduction} in
the manner described in~\cite{Duchi2015Information}, we can split the minimax
risk along each of the $d$ dimensions, and bound each separately using Le Cam's
method.

\section{Discussion}
\label{sec:discussion}

The lower bound given by Theorem~\ref{thm:main-lb} goes to zero as the number
of sensors, $m$, goes to infinity, \emph{even if} the total number of trials
$n$ is kept \emph{fixed}. This would mean that a high sensor density can
overcome the effect of noise in a single trial. The reason this happens is that
we assume sensors are affected by i.i.d. noise, no matter what sampling density
we use. This is an increasingly poor assumption as sensor density increases,
however; we expect noise to become correlated if sensors are extremely close
(due to inductive/capacitive coupling, for example). A more realistic lower
bound will show the benefit of increasing sensor density up to a point only,
beyond which it will have to scale alongside the number of trials.

The lower bounding \emph{technique} gives certain inisights on the issue of
sensor placement. Suppose we wanted to achieve a higher resolution in a
restricted interval of the parameter space, i.e.\ $\theta \in [a, b] \subset
\Theta$, such that $\abs{b - a} \ll w$. Then, it might be of greatest benefit
to place sensors \emph{not} precisely over the region, but rather in the region
corresponding to the \emph{largest change} in the impulse response. Indeed, it
is possible to derive an  approximate lower bound using Taylor's approximation,
which shows that the KL-divergence between distributions is maximized (and
hence error minimized) at regions where the slope of $x(s;\theta)$ is largest.

Another point on sensor placement bears repeating: the bounds derived in this
paper apply to only a specific sensor configuration, i.e., the uniform one.
This would intuitively seem to be an appropriate choice in the minimax setting,
however, since any non-uniform placement will have one or more points which is
far from all sensors. The ``maximization'' over $\theta \in \Theta$ would then
select such a point, where the value of the impulse response so produced at the
sensors is smallest, so that SNR is minimized.

For the application of EEG source localization, it might be possible to find
algorithms which achieve similar scalings as the lower bounds presented here.
It is also hoped that the techniques presented here will help us derive
analytical bounds in the more difficult setting.

\section*{Acknowledgements}

Praveen Venkatesh is partially supported by the Dowd Fellowship from the
College of Engineering at Carnegie Mellon University. The authors would like to
thank Philip and Marsha Dowd for their support and encouragement.

%\IEEEtriggeratref{8}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
